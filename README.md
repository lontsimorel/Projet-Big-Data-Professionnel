Ce projet est une implÃ©mentation professionnelle de bout en bout dâ€™une architecture Big Data moderne, allant :

â†’ de lâ€™ingestion et la prÃ©paration de donnÃ©es massives,
â†’ Ã  lâ€™analyse exploratoire distribuÃ©e (EDA),
â†’ Ã  lâ€™entraÃ®nement de modÃ¨les Machine Learning Ã  grande Ã©chelle,
â†’ jusqu'Ã  la containerisation complÃ¨te du workflow avec Docker pour garantir reproductibilitÃ©, scalabilitÃ© et portabilitÃ©.

Lâ€™objectif est de dÃ©montrer une maÃ®trise complÃ¨te de la chaÃ®ne de valeur Data, dans un contexte proche dâ€™un environnement industriel.

ğŸ§± Architecture du projet

Le projet repose sur une architecture distribuÃ©e orchestrÃ©e via Docker Compose, comprenant :

ğŸ”¹ 1. Spark Master & Workers

Environnement de traitement distribuÃ© basÃ© sur Apache Spark :

Spark Master

Plusieurs Spark Workers

Communication interne via le cluster Spark Standalone

Support de Spark MLlib, DataFrame API, SQL, PCA, clustering, etc.

ğŸ”¹ 2. JupyterLab avec PySpark intÃ©grÃ©

Un environnement interactif totalement intÃ©grÃ© au cluster :

Preload de PySpark, pandas, numpy, seaborn, matplotlib

Connexion automatique au Spark Master

Montage automatique des rÃ©pertoires /Data, /Notebooks, /Modeles

ğŸ”¹ 3. Structure du projet
ğŸ“¦ Projet_BigData
â”œâ”€â”€ Data/                â†’ datasets sources + parquet
â”œâ”€â”€ Notebooks/           â†’ EDA + feature engineering + modÃ¨les ML
â”œâ”€â”€ Modeles/             â†’ modÃ¨les entraÃ®nÃ©s (Pickle, MLlib)
â”œâ”€â”€ docker-compose.yml   â†’ orchestration complÃ¨te
â”œâ”€â”€ Dockerfile.spark     â†’ image Spark Master/Worker
â”œâ”€â”€ Dockerfile.jupyter   â†’ image JupyterLab + PySpark
â”œâ”€â”€ spark/               â†’ configuration avancÃ©e (workers, env, defaults)
â””â”€â”€ README.md

ğŸ“Š Traitement & Analyse des DonnÃ©es

Les donnÃ©es brutes (Open Data Enedis) sont prÃ©traitÃ©es dans Spark :

nettoyage et harmonisation

gestion des valeurs manquantes

typage automatique (numÃ©rique/catÃ©goriel)

enrichissement mÃ©tier

export en Parquet optimisÃ©

Un EDA complet a Ã©tÃ© rÃ©alisÃ© en mode distribuÃ© :

statistiques globales

corrÃ©lations & heatmaps

analyse temporelle

distribution des consommations

ğŸ¤– Machine Learning Ã  grande Ã©chelle (Spark MLlib)

Plusieurs familles de modÃ¨les ont Ã©tÃ© entraÃ®nÃ©es :

ğŸ”¹ RÃ©gression (prÃ©diction de consommation)

Random Forest Regressor

Linear Regression

Gradient Boosted Trees Regressor

Avec :

Pipeline complet : encodage, assemblage, scaling

GridSearch distribuÃ©

Cross-validation Spark

mÃ©triques (RMSE, MAE, RÂ²)

Chaque modÃ¨le est sauvegardÃ© automatiquement dans /Modeles.

ğŸ”¹ Classification

Random Forest Classifier

GBT Classifier

Decision Tree Classifier

MÃ©triques obtenues :
â†’ Accuracy, F1-score, Precision, Recall

ğŸ”¹ Clustering & RÃ©duction de dimension

PCA (visualisation et compression)

KMeans (segmentation des profils de consommation)

ğŸ³ Containerisation & DÃ©ploiement â€“ Docker

Le projet est entiÃ¨rement containerisÃ© pour Ãªtre reproductible sur Windows, Mac ou Linux.

Docker Compose dÃ©ploie automatiquement :

Spark Master

Spark Workers

JupyterLab

Volumes persistants (data/models/notebooks)

Configuration Spark centralisÃ©e

Ce qui permet :

âœ” ReproductibilitÃ© totale
âœ” Aucune installation locale complexe
âœ” ScalabilitÃ© (ajout de workers en 1 ligne)
âœ” Isolation des environnements

ğŸ“¦ Technologies utilisÃ©es
Domaine	Outils
Big Data	Apache Spark 4.x, Hadoop 3.3
Machine Learning	Spark MLlib, PySpark
Visualisation	Matplotlib, Seaborn
Containerisation	Docker, Docker Compose
Workflow	JupyterLab
Stockage	Parquet
Dev	Python 3, Git
ğŸ¯ Objectifs du projet

Construire un pipeline Big Data complet et industrialisable

Exploiter Spark pour traiter des datasets volumineux

Fournir un framework ML reproductible via Docker

Proposer une architecture prÃªte au dÃ©ploiement cloud (AWS / GCP / Azure)
